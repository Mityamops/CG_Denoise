{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient based optimization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробовать изображение с несколькими обьектами\n",
    "# соты\n",
    "# соты с дырками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import notebook_importer\n",
    "#from utility import *\n",
    "import cv2\n",
    "import random \n",
    "import numpy as np\n",
    "from numpy import sqrt, sum, abs, max, maximum, logspace, exp, log, log10, zeros\n",
    "from numpy.linalg import norm\n",
    "from numpy.random import randn, rand\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "from scipy import optimize\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "### Lipschitz Estimator\n",
    "A method that estimates the Lipschitz constant of a function $g$.  This can be done using the formula\n",
    "$$L \\approx \\frac{\\|g(x) - g(y)\\|}{\\|x-y\\|}.$$\n",
    "The inputs should be a function $g:\\mathbb{R}^n \\to \\mathbb{R}^m,$ and an initial vector $x$ in the domain of $g$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_lipschitz(g, x):\n",
    "    y = rand(*x.shape)\n",
    "    L = norm(g(x)-g(y))/norm(x-y)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A routine that minimizes a function using gradient descent.\n",
    "The inputs $f$ and $grad$ are function handles.  The function $f: \\mathbb{R}^N\\to \\mathbb{R}$ is an arbitrary objective function, and  $grad: \\mathbb{R}^N \\to \\mathbb{R}^N$ is its gradient.  The method minimizes $f$ using gradient descent, and terminate when the gradient of $f$ is small.  \n",
    "\n",
    "**Stopping condition:**\n",
    "$$\\|\\nabla f(x^k)\\|<\\|\\nabla f(x^0)\\|*tol$$\n",
    " where $x^0$ is an initial guess and $tol$ is a small tolerance parameter (a typical value would be $10^{-4}$).  \n",
    " \n",
    "  Use a backtracking line search to guarantee convergence.   The stepsize should be monotonically decreasing.  Each iteration should begin by trying the stepsize that was used on the previous iteration, and then backtrack until the Armijo condition holds:\n",
    "  $$f(x^{k+1}) \\le f(x^k) + \\alpha \\langle x^{k+1} - x^k, \\nabla f(x^k)\\rangle,$$\n",
    "  where $\\alpha \\in (0,1),$ and $\\alpha=0.1$ is suggested.\n",
    "\n",
    "  The function returns the solution vector $x_{sol}$, and also a vector $res$ containing the norm of the residual (i.e., the norm of the gradient) at each iteration.\n",
    "\n",
    "This initial stepsize should be $10/L$, where $L$ is an estimate of the Lipschitz constant for the gradient. We over-estimate the step size intially and tone it down using the line search condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(f, grad, x0, max_iters=10000, tol=1e-4):\n",
    "    x_k = x0\n",
    "    L = estimate_lipschitz(grad, x_k)\n",
    "    step_size = 10/L\n",
    "    res=[]\n",
    "    res.append(norm(grad(x_k)))\n",
    "    d = -grad(x0)\n",
    "    while (np.linalg.norm(d)>tol):\n",
    "        d = -grad(x_k)\n",
    "\n",
    "        while(f(x_k + step_size*d)>=(f(x_k) + 0.1*(np.sum((step_size*d)*(-d))))):\n",
    "            step_size = step_size/2\n",
    "        x_k1 = x_k + step_size*d  \n",
    "        res.append(norm((-d)))\n",
    "        \n",
    "        if res[-1] < tol*res[0]:\n",
    "            x= x_k\n",
    "            break\n",
    "        x_k = x_k1     \n",
    "        \n",
    "    return x, res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient solver that begins each iteration using a Barzilai-Borwein stepsize (BB Method) \n",
    "  $$\\tau = \\frac{\\langle x^{k+1} - x^k ,x^{k+1} - x^k   \\rangle}{\\langle x^{k+1} - x^k ,\\nabla f(x^{k+1}) - \\nabla f(x^k)   \\rangle}.$$\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent_bb(f, grad, x0, max_iters=10000, tol=1e-4):\n",
    "    x_k = x0\n",
    "    L = estimate_lipschitz(grad, x0)\n",
    "    step_size = L/100\n",
    "    x_k1= x_k - step_size*grad(x_k)\n",
    "    res = []\n",
    "    res.append(norm(grad(x_k)))\n",
    "    d = -grad(x_k)\n",
    "    while (np.linalg.norm(d)>tol):\n",
    "        \n",
    "        step_size = (np.sum((x_k1-x_k)*(x_k1 - x_k)))/(np.sum((x_k1 - x_k)*(grad(x_k1) +d)))        \n",
    "\n",
    "        \n",
    "        while(f(x_k + step_size*d)>=(f(x_k) + 0.1*step_size*(np.sum((d)*(-d))))):\n",
    "            step_size = step_size/2        \n",
    "\n",
    "        x_k = x_k1\n",
    "        d = -grad(x_k)\n",
    "        res.append(norm(d))\n",
    "        x_k1 = x_k + step_size*d\n",
    "\n",
    "        if(res[-1]<tol*res[0]):\n",
    "            x= x_k\n",
    "            break\n",
    "        \n",
    "    \n",
    "    return x, res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A routine that uses Nesterov's accelerated gradient method\n",
    "\\begin{align}\n",
    "x^{k} &= y^k - \\tau \\nabla f(y^k)\\\\\n",
    "\\delta^{k+1} &= \\frac{1+\\sqrt{1+4(\\delta^k)^2}}{2}\\\\\n",
    "y^{k+1} &= x^{k}+\\frac{\\delta^k-1}{\\delta^{k+1}}(x^k-x^{k-1})\n",
    "\\end{align}\n",
    "The stepsize restriction for Nesterov's methods is $\\tau<1/L,$ however when $L$ is not known exactly you can use the line search condition\n",
    " $$f(x^k) \\le f(y^{k}) + \\alpha (x^k-y^k)^T\\nabla f(y^k), $$\n",
    " where $\\alpha \\in [1/2,1).$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent_nesterov(f, grad, x0, max_iters=10000, tol=1e-4):\n",
    "    L = estimate_lipschitz(grad, x0)\n",
    "    step_size = 0.8/L\n",
    "    delta_k =1 \n",
    "    res = []\n",
    "    y_k = x0\n",
    "    x_k_prev = x0\n",
    "    x_k = y_k - step_size*grad(y_k)\n",
    "    res.append(norm(grad(x_k_prev)))\n",
    "    while (np.linalg.norm(grad(x_k))>tol):\n",
    "       \n",
    "        d=-grad(y_k)\n",
    "        #Armijo condition\n",
    "        while(f(y_k+step_size*d)>=(f(y_k) + 0.5*(np.sum((step_size*d)*(-d))))):\n",
    "            step_size = step_size/2        \n",
    "        \n",
    "        \n",
    "        #Calculation for updates\n",
    "        x_k = y_k - step_size*(-d)\n",
    "        delta_k1 = (1 + np.sqrt(4*(delta_k**2) + 1))/2\n",
    "        y_k1 = x_k + ((delta_k - 1)/(delta_k1))*(x_k - x_k_prev)\n",
    "        \n",
    "        #Updates\n",
    "        delta_k = delta_k1\n",
    "        x_k_prev = x_k\n",
    "        y_k = y_k1\n",
    "        res.append(norm(grad(x_k)))\n",
    "        \n",
    "        #Check for convergence\n",
    "        if res[-1] < tol*res[0]:\n",
    "            x= x_k\n",
    "            break\n",
    "      \n",
    "    return x, res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fletcher-Reeves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FR(f,grad,x0,e=0.001):\n",
    "    \n",
    "    xcur = np.array(x0)\n",
    "    n = len(x0)\n",
    "    k = 0 # step1\n",
    "    dk=grad(x0)\n",
    "    prevgrad = 1\n",
    "    pk = -1*dk\n",
    "    res=[]\n",
    "    res.append(norm(grad(x0)))\n",
    "    #while (k<1000): # step3\n",
    "    while (np.linalg.norm(dk)>e): # step3\n",
    "        if (k%n==0): # step4\n",
    "            pk = -1*dk\n",
    "        else:\n",
    "            bk = (np.linalg.norm(dk)**2)/(np.linalg.norm(prevgrad)**2) # step5\n",
    "            prevpk = pk\n",
    "            pk = -1*dk + bk*prevpk # step6\n",
    "        a = (optimize.minimize_scalar(lambda x: f(xcur+pk*x), bounds=(0,10)).x)\n",
    "        xcur = xcur + a*pk #step8\n",
    "        k=k+1 #step8\n",
    "        prevgrad=dk\n",
    "        dk=grad(xcur)\n",
    "        res.append(norm(dk))\n",
    "    return xcur,res,k #step10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HY(f,grad,x0,e=0.001):\n",
    "    xcur = np.array(x0)\n",
    "    n = len(x0)\n",
    "    k = 0 # step1\n",
    "    dk=grad(x0)\n",
    "    prevgrad = 1\n",
    "    pk = -1*dk\n",
    "    res=[]\n",
    "    res.append(norm(grad(x0)))\n",
    "    while (np.linalg.norm(dk)>e): # step3\n",
    "        a = (optimize.minimize_scalar(lambda x: f(xcur+pk*x), bounds=(0,100)).x)\n",
    "        xpred=xcur\n",
    "        xcur = xcur + a*pk #step8\n",
    "        if (k%n==0): # step4\n",
    "            pk = -1*dk\n",
    "        else:\n",
    "            bk = (np.linalg.norm(dk)**2) / (np.dot(np.squeeze(-dk+prevgrad), np.squeeze(pk)))\n",
    "            prevpk = pk\n",
    "            pk = -1*dk + bk*prevpk # step6\n",
    "        \n",
    "        \n",
    "        k=k+1 #step8\n",
    "        prevgrad=dk\n",
    "        dk=grad(xcur)\n",
    "        res.append(norm(dk))\n",
    "    return xcur,res #step10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image denoising\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Variation (TV) objective to denoise an image\n",
    "\n",
    "**TV objective**\n",
    "$$ |\\nabla x| + \\frac{\\mu}{2}\\|x-f\\|^2$$\n",
    "\n",
    "Since L1 norm is not differentiable, we use hyperbolic regularization where\n",
    "$$ h(x) = \\sqrt{x^2+\\epsilon ^2}$$\n",
    "\n",
    "The objective becomes:\n",
    "$$h(\\nabla x) + \\frac{\\mu}{2}\\|x-f\\|^2$$\n",
    "\n",
    "The derivative of this objective is :\n",
    "$$ \\nabla^{T}h'(\\nabla x) + \\mu(x-f)$$\n",
    "\n",
    "where $$ h'(x) = \\frac{x}{\\sqrt{x^2 + \\epsilon^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of the individual components of the objective\n",
    "kernel_h = [[1,-1,0]] \n",
    "kernel_v = [[1],[-1],[0]]\n",
    "\n",
    "def gradh(x):\n",
    "    \"\"\"Discrete gradient/difference in horizontal direction\"\"\"\n",
    "    return convolve2d(x,kernel_h, mode='same', boundary='wrap')\n",
    "def gradv(x):\n",
    "    \"\"\"Discrete gradient/difference in vertical direction\"\"\"\n",
    "    return convolve2d(x,kernel_v, mode='same', boundary='wrap')\n",
    "def grad2d(x):\n",
    "    \"\"\"The full gradient operator: compute both x and y differences and return them all.  The x and y \n",
    "    differences are stacked so that rval[0] is a 2D array of x differences, and rval[1] is the y differences.\"\"\"\n",
    "    return np.stack([gradh(x),gradv(x)])\n",
    "\n",
    "def gradht(x):\n",
    "    \"\"\"Adjoint of gradh\"\"\"\n",
    "    kernel_ht = [[0,-1,1]] \n",
    "    return convolve2d(x,kernel_ht, mode='same', boundary='wrap')\n",
    "def gradvt(x):\n",
    "    \"\"\"Adjoint of gradv\"\"\"\n",
    "    kernel_vt = [[0],[-1],[1]]\n",
    "    return convolve2d(x,kernel_vt, mode='same', boundary='wrap')\n",
    "def divergence2d(x):\n",
    "    \"The method is the adjoint of grad2d.\"\n",
    "    return gradht(x[0])+gradvt(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "def h(z, eps=.01):\n",
    "    \"\"\"The hyperbolic approximation to L1\"\"\"\n",
    "    return sum(sqrt(z*z+eps*eps).ravel())\n",
    "def tv_denoise_objective(x,mu,b):\n",
    "    return h(grad2d(x)) + 0.5*mu*norm(x-b)**2\n",
    "def h_grad(z, eps=.01):\n",
    "    \"\"\"The gradient of h\"\"\"\n",
    "    return z/sqrt(z*z+eps*eps)\n",
    "def tv_denoise_grad(x,mu,b):\n",
    "    \"\"\"The gradient of the TV objective\"\"\"\n",
    "    return divergence2d(h_grad(grad2d(x))) + mu*(x-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the BB routine above to minimize the TV objective, and denoise the test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a noisy image\n",
    "image = zeros((50,50)).astype(np.uint8)\n",
    "image[15:35,15:35]=1\n",
    "#image = image+1*randn(50,50)\n",
    "gauss_noise=np.zeros((image.shape[0],image.shape[1]),dtype=np.uint8)\n",
    "cv2.randn(gauss_noise,128,20)\n",
    "gauss_noise=(gauss_noise*0.03).astype(np.uint8)\n",
    "image=cv2.add(image,gauss_noise)\n",
    "\n",
    "\n",
    "plt.title('Noisy image')\n",
    "plt.imshow(image,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a noisy image\n",
    "image = zeros((200,200)).astype(np.uint8)\n",
    "image[60:140,60:140]=1\n",
    "image = image+1*randn(200,200)\n",
    "#gauss_noise=np.zeros((image.shape[0],image.shape[1]),dtype=np.uint8)\n",
    "#cv2.randn(gauss_noise,128,20)\n",
    "#gauss_noise=(gauss_noise*0.03).astype(np.uint8)\n",
    "#image=cv2.add(image,gauss_noise)\n",
    "\n",
    "\n",
    "plt.title('Noisy image')\n",
    "plt.imshow(image,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.zeros(image.shape)\n",
    "f = lambda x: tv_denoise_objective(x, mu=0.1, b=image)\n",
    "grad = lambda x: tv_denoise_grad(x, mu=0.1, b=image)\n",
    "#x= FR(f, grad, x0,0.001)\n",
    "x, res = grad_descent_bb(f, grad, x0)\n",
    "plt.title(\"Denoised image\")\n",
    "plt.imshow(x,cmap='gray')\n",
    "plt.show()\n",
    "print(f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x)# 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.zeros(image.shape)\n",
    "f = lambda x: tv_denoise_objective(x, mu=0.3, b=image)\n",
    "grad = lambda x: tv_denoise_grad(x, mu=0.3, b=image)\n",
    "x,_,k= FR(f, grad, x0,0.001)\n",
    "print(k)\n",
    "#x, res = grad_descent_bb(f, grad, x0)\n",
    "plt.title(\"Denoised image\")\n",
    "plt.imshow(x,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impulse_noise(image, prob):\n",
    "    noisy_image = np.copy(image)\n",
    "    salt_and_pepper = np.random.rand(*image.shape)\n",
    "    noisy_image[salt_and_pepper < prob] = 0\n",
    "    noisy_image[salt_and_pepper > 1 - prob] = 255\n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a noisy image\n",
    "Lena = cv2.imread('Lena.png')\n",
    "Lena=cv2.cvtColor(Lena, cv2.COLOR_BGR2GRAY)\n",
    "m,n=Lena.shape\n",
    "\n",
    "Lena_noise = Lena+40*randn(m,n)\n",
    "\n",
    "#Lena_noise=impulse_noise(Lena,0.03)\n",
    "\n",
    "plt.title('Noisy image')\n",
    "plt.imshow(Lena_noise,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.zeros(Lena.shape)\n",
    "f = lambda x: tv_denoise_objective(x, mu=0.01, b=Lena_noise)\n",
    "grad = lambda x: tv_denoise_grad(x, mu=0.01, b=Lena_noise)\n",
    "x,res,_= FR(f, grad, x0,0.001)\n",
    "#x, res = grad_descent_bb(f, grad, x0,0.000001)\n",
    "plt.title(\"Denoised image\")\n",
    "plt.imshow(x,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.zeros(Lena.shape)\n",
    "f = lambda x: tv_denoise_objective(x, mu=0.02, b=Lena_noise)\n",
    "grad = lambda x: tv_denoise_grad(x, mu=0.02, b=Lena_noise)\n",
    "x,res,_= FR(f, grad, x0,0.001)\n",
    "#x, res = grad_descent_bb(f, grad, x0,0.000001)\n",
    "plt.title(\"Denoised image\")\n",
    "plt.imshow(x,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.zeros(Lena.shape)\n",
    "f = lambda x: tv_denoise_objective(x, mu=0.03, b=Lena_noise)\n",
    "grad = lambda x: tv_denoise_grad(x, mu=0.03, b=Lena_noise)\n",
    "x,res,_= FR(f, grad, x0,0.001)\n",
    "#x, res = grad_descent_bb(f, grad, x0,0.000001)\n",
    "plt.title(\"Denoised image\")\n",
    "plt.imshow(x,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.zeros(Lena.shape)\n",
    "f = lambda x: tv_denoise_objective(x, mu=0.05, b=Lena_noise)\n",
    "grad = lambda x: tv_denoise_grad(x, mu=0.05, b=Lena_noise)\n",
    "#x,res,_= FR(f, grad, x0,0.001)\n",
    "x, res = grad_descent_bb(f, grad, x0,0.001)\n",
    "plt.title(\"Denoised image\")\n",
    "plt.imshow(x,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x.astype(np.uint8)\n",
    "Lena_noise=Lena_noise.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr = cv2.PSNR(x, Lena_noise)\n",
    "psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impulse_noise(image, prob):\n",
    "    noisy_image = np.copy(image)\n",
    "    salt_and_pepper = np.random.rand(*image.shape)\n",
    "    noisy_image[salt_and_pepper < prob] = 0\n",
    "    noisy_image[salt_and_pepper > 1 - prob] = 255\n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FR_1(f,grad,x0,e=0.001):\n",
    "    \n",
    "    xcur = np.array(x0)\n",
    "    n = len(x0)\n",
    "    k = 0 # step1\n",
    "    dk=grad(x0)\n",
    "    prevgrad = 1\n",
    "    pk = -1*dk\n",
    "    res=[]\n",
    "    \n",
    "    prevx=0\n",
    "    #while (np.linalg.norm(prevx-x)>e): # step3\n",
    "    while (k<1000): # step3\n",
    "        if (k%n==0): # step4\n",
    "            pk = -1*dk\n",
    "        else:\n",
    "            bk = (np.linalg.norm(dk)**2)/(np.linalg.norm(prevgrad)**2) # step5\n",
    "            prevpk = pk\n",
    "            pk = -1*dk + bk*prevpk # step6\n",
    "        a = (optimize.minimize_scalar(lambda x: f(xcur+pk*x), bounds=(0,10)).x)\n",
    "        prevx=xcur\n",
    "        xcur = xcur + a*pk #step8\n",
    "        \n",
    "        k=k+1 #step8\n",
    "        prevgrad=dk\n",
    "        dk=grad(xcur)\n",
    "        res.append(xcur)\n",
    "    return xcur,res,k #step10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a noisy image\n",
    "img = zeros((50,50)).astype(np.uint8)\n",
    "img[15:35,15:35]=1\n",
    "#image = image+1*randn(50,50)\n",
    "gauss_noise=np.zeros((img.shape[0],img.shape[1]),dtype=np.uint8)\n",
    "cv2.randn(gauss_noise,128,20)\n",
    "gauss_noise=(gauss_noise*0.02).astype(np.uint8)\n",
    "img=cv2.add(img,gauss_noise)\n",
    "\n",
    "\n",
    "plt.title('Noisy image')\n",
    "plt.imshow(img,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.zeros(img.shape)\n",
    "f = lambda x: tv_denoise_objective(x, mu=0.3, b=img)\n",
    "grad = lambda x: tv_denoise_grad(x, mu=0.3, b=img)\n",
    "x,results,k= FR_1(f, grad, x0,0.001)\n",
    "\n",
    "for res in results:\n",
    "    plt.title(\"Denoised image\")\n",
    "    plt.imshow(res,cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
